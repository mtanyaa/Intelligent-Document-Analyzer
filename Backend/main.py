# -*- coding: utf-8 -*-
"""main.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OciXDNlJkbect9aof1H-yldxE8X76S-O
"""

import os, re, cv2, json, numpy as np, shutil, urllib.request
from datetime import datetime
from pathlib import Path
from typing import List

from fastapi import FastAPI, UploadFile, File, Form
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse

from PIL import Image
import pytesseract
from pdf2image import convert_from_path
from ultralytics import YOLO
from sentence_transformers import SentenceTransformer, util


# === Paths ===
BASE_DIR = Path(__file__).parent
INPUT_PDF_DIR = BASE_DIR / "input_pdfs"
INTERMEDIATE_DIR = BASE_DIR / "outputs"
FINAL_OUTPUT_PATH = BASE_DIR / "semantic_filtered_output.json"
MODEL_PATH = BASE_DIR / "yolov8n-doclaynet.pt"

INPUT_PDF_DIR.mkdir(exist_ok=True)
INTERMEDIATE_DIR.mkdir(exist_ok=True)

# === Download YOLO model if missing ===
if not MODEL_PATH.exists():
    MODEL_URL = "https://huggingface.co/hantian/yolo-doclaynet/resolve/main/yolov8n-doclaynet.pt"
    print("Downloading YOLO model...")
    urllib.request.urlretrieve(MODEL_URL, MODEL_PATH)

# === Load models once ===
print("Loading YOLO + SentenceTransformer...")
yolo_model = YOLO(str(MODEL_PATH))
sem_model = SentenceTransformer("all-MiniLM-L12-v2")


# === Helper Functions ===
def is_valid_heading(text, is_title=False):
    if not text:
        return False
    text = text.strip()
    if not is_title and len(text.split()) > 12:
        return False
    if any(text.lower().startswith(k) for k in ["page", "version", "copyright", "may", "june", "july", "august"]):
        return False
    if re.match(r"^\d{1,2}/\d{1,2}/\d{2,4}$", text):
        return False
    if re.match(r"^\d+[\.\)]$", text):
        return False
    if not re.search(r"[a-zA-Z]", text):
        return False
    return True


def extract_pdf(pdf_path: Path):
    pages = convert_from_path(str(pdf_path), dpi=300)
    results = []
    title = None

    for page_idx, pil_img in enumerate(pages, start=1):
        img = cv2.cvtColor(np.array(pil_img), cv2.COLOR_RGB2BGR)
        det = yolo_model(img)[0]

        ocr_data = pytesseract.image_to_data(img, config="--psm 6", output_type=pytesseract.Output.DICT)
        words_with_pos = []
        for i in range(len(ocr_data["text"])):
            text = ocr_data["text"][i].strip()
            if text:
                words_with_pos.append({
                    "text": text,
                    "x": int(ocr_data["left"][i]),
                    "y": int(ocr_data["top"][i]),
                    "w": int(ocr_data["width"][i]),
                    "h": int(ocr_data["height"][i]),
                })

        heading_boxes = []
        for box in det.boxes:
            cls = int(box.cls[0])
            label = yolo_model.names[cls].lower()
            if label not in ("title", "section-header"):
                continue

            x1, y1, x2, y2 = map(int, box.xyxy[0])
            crop = img[y1:y2, x1:x2]
            crop_text = pytesseract.image_to_string(crop, config="--psm 6").strip()

            is_title = page_idx == 1 and title is None and (label == "title")
            if is_valid_heading(crop_text, is_title=is_title):
                if is_title:
                    title = crop_text
                else:
                    heading_boxes.append({
                        "text": crop_text,
                        "crop_text": crop_text,
                        "bbox": [int(x1), int(y1), int(x2), int(y2)],
                        "y": y1
                    })

        heading_boxes.sort(key=lambda b: b["y"])

        for idx, heading in enumerate(heading_boxes):
            y_start = heading["y"]
            y_end = heading_boxes[idx + 1]["y"] if idx + 1 < len(heading_boxes) else img.shape[0]

            span_text = " ".join(
                w["text"] for w in words_with_pos if y_start <= w["y"] < y_end
            )

            heading["page"] = page_idx
            heading["raw_page_text"] = f"{heading['crop_text']}\n{span_text}"
            del heading["y"]
            results.append(heading)

    if title is None:
        title = results[0]["text"] if results else "Untitled Document"

    return {"title": title, "outline": results}


def run_pipeline(pdf_paths: List[Path], persona: str, job_to_be_done: str):
    job_embedding = sem_model.encode(job_to_be_done, convert_to_tensor=True)

    metadata = {
        "input_documents": [],
        "persona": persona,
        "job_to_be_done": job_to_be_done,
        "processing_timestamp": datetime.now().isoformat()
    }
    extracted_sections = []
    subsection_analysis = []

    for path in pdf_paths:
        data = extract_pdf(path)
        (INTERMEDIATE_DIR / f"{path.stem}_headings_plus_text.json").write_text(
            json.dumps(data, indent=2, ensure_ascii=False),
            encoding="utf-8"
        )

        input_filename = path.name
        metadata["input_documents"].append(input_filename)

        sections = [
            (section, section.get("text", "").strip().lower(), section.get("raw_page_text", "").strip())
            for section in data.get("outline", [])
            if section.get("text", "").strip() and section.get("raw_page_text", "").strip()
               and len(section.get("raw_page_text", "").split()) >= 10
        ]
        if not sections:
            continue

        headings = [title for _, title, _ in sections]
        heading_embeddings = sem_model.encode(headings, convert_to_tensor=True, show_progress_bar=False)

        candidate_sections = []
        similarities = []
        for (section, title, raw_text), heading_emb in zip(sections, heading_embeddings):
            similarity = float(util.cos_sim(job_embedding, heading_emb)[0][0])
            similarities.append(similarity)
            candidate_sections.append((similarity, section, raw_text))

        max_similarity = max(similarities) if similarities else 0.0
        normalized_similarities = [s / max_similarity if max_similarity > 0 else s for s in similarities]
        SIMILARITY_THRESHOLD = max(0.6, min(np.percentile(normalized_similarities, 75), 0.8))

        filtered = [
            (score, section, text)
            for (score, section, text), norm_score in zip(candidate_sections, normalized_similarities)
            if norm_score >= SIMILARITY_THRESHOLD
        ]
        filtered.sort(reverse=True, key=lambda x: x[0])

        for rank, (score, section, refined_text) in enumerate(filtered[:5], start=1):
            extracted_sections.append({
                "document": input_filename,
                "section_title": section["text"],
                "importance_rank": rank,
                "page_number": section["page"]
            })
            subsection_analysis.append({
                "document": input_filename,
                "refined_text": refined_text.replace("\n", " ").strip(),
                "page_number": section["page"]
            })

    final_output = {
        "metadata": metadata,
        "extracted_sections": extracted_sections,
        "subsection_analysis": subsection_analysis
    }
    FINAL_OUTPUT_PATH.write_text(json.dumps(final_output, indent=2, ensure_ascii=False), encoding="utf-8")
    return final_output


# === FastAPI App ===
app = FastAPI(title="PDF Semantic Extractor")
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"], allow_credentials=True, allow_methods=["*"], allow_headers=["*"]
)


@app.post("/process")
async def process(
    files: List[UploadFile] = File(...),
    persona: str = Form(...),
    job_to_be_done: str = Form(...)
):
    # Clean input folder
    for p in INPUT_PDF_DIR.glob("*.pdf"):
        p.unlink()

    saved_paths = []
    for f in files:
        if not f.filename.lower().endswith(".pdf"):
            continue
        dest = INPUT_PDF_DIR / f.filename
        with dest.open("wb") as out:
            shutil.copyfileobj(f.file, out)
        saved_paths.append(dest)

    if not saved_paths:
        return JSONResponse({"error": "No PDF files uploaded"}, status_code=400)

    result = run_pipeline(saved_paths, persona, job_to_be_done)
    return JSONResponse(result)